%% LyX 2.3.5.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{setspace}
\onehalfspacing
\usepackage{babel}
\begin{document}
\title{CM 763: Paper Review}
\maketitle

\subsection*{A Brief Comment on Our Review}

The paper we are reviewing relies on proofs that are both technical
and lengthy. The authors use 9 lemmas spanning order statistics, random
matrix theory and probability theory in order to prove the main results.
Furthermore, the authors provide 4 pruning techniques (with corresponding
proofs). However these proofs are all quite similar. Consequently,
we will not present the technical details of the proof, since we feel
this would be too long and too complicated for a (relatively) short
review. Instead, we hope to:
\begin{enumerate}
\item Clarify the terminology used by the authors.
\item Explain how this terminology intuitively describes pruning technique.
\item Explain why the assumptions made in the magnitude based pruning technique
are required.
\item Briefly comment on the validity of these assumptions.
\end{enumerate}

\subsection*{Clarifying the Objects of Study}

In \cite{qian2021probabilistic} the authors define the following
standard objects and notation:
\begin{itemize}
\item $L+1$ denotes the depth the neural network (alternatively there are
$L$ hidden layers), with $l\in\{1,...,L\}$ indexing hidden layers
(the input layer is denoted with $l=0$).
\item $d_{l}$ denotes the number of nodes in the hidden layer $l\in\{1,...,L\}$.
\item Let $v\in\mathbb{R}^{d_{l}}$. The $L_{0}$ norm of $v$ is denoted
$||v||_{0}$, and specifies the number of non-zero entries in $v$.
The $L_{2}$ norm of $v$ is denotes $||v||_{2}$, which denotes the
Euclidean ``standard'' norm $||v||_{2}=\sqrt{v^{T}v}$.
\item Let $A\in\mathbb{R}^{m\times n}$ . The vectorization of $A$ is defined
as follows:
\[
\text{vec}(A)=\left[A_{1,1},...,A_{1,n},...,A_{m,1},...,A_{m,n}\right]^{T}\in\mathbb{R}^{m\cdot n}
\]
where we use $``\cdot"$ to dinstinguish $\mathbb{R}^{m\cdot n}$
(a set of vectors with dimension $m\cdot n$) from $\mathbb{R}^{m\times n}$
(the set of linear operators $V$ such that $V:\mathbb{R}^{m}\rightarrow\mathbb{R}^{n})$. 
\item Let $A\in\mathbb{R}^{m\times n}$ , then the induced 2-norm of $A$
is defined as 
\[
||A||_{2}=\sigma_{\max}(A)
\]
where $\sigma_{\max}(A)$ denotes the largest singular value of $A$.
\item If $A,B\in\mathbb{R}^{m\times n}$ then $A\circ B$ denotes the Hadamard
product so that $\left[A\circ B\right]_{ij}=A_{ij}B_{ij};\ i\in\{1,...,m\},\ j\in\{1,...,n\}$. 
\item $\mathcal{U}[a,b]$ denotes the uniform distribution on $[a,b]$ and
$\mathcal{N}(\mu,\Sigma)$ denotes the multivariate normal distribution.
\item $\sigma_{l}:\mathbb{R\rightarrow\mathbb{R}},\ l\in\{1,...,L\}$ is
the activation function associated with layer $l$. $W_{l}^{*}\in\mathbb{R}^{d_{l}\times d_{l-1}},\ l\in\{0,...,L\}$
denotes the weight matrix of this layer. 
\item The target network is defined as $F(x)=W_{l}^{*}\sigma_{l-1}(W_{l-1}^{*}\sigma_{l-1}(\cdots W_{2}^{*}\sigma_{1}(W_{1}^{*}x)))$
and is just the functional representation of a neural network. It
is assumed activation functions act componentwise on their inputs.
\end{itemize}
The authors in \cite{qian2021probabilistic} also define the some
more idiosyncratic objects:
\begin{itemize}
\item The number of weights in the $l^{\text{th}}$ layer is denoted $D_{l}:=d_{l}d_{l-1}$.
\item A mask matrix $M\in[0,1]^{m\times n}$ is just an $m\times n$ valued
matrix whose entries are identically $0$ or $1$.
\item A pruned weight matrix corresponding to the weight matrix $W_{l}^{*}$
of a fully connected network is given by $W_{l}=M_{l}\circ W_{l}^{*}$
where $M_{l}$ is a mask matrix.
\item A pruning $f(x)$ of the target network $F(x)$ is defined as $f(x)=W_{l}\sigma_{l-1}(W_{l-1}\sigma_{l-1}(\cdots W_{2}\sigma_{1}(W_{1}x)))$. 
\item Let $W_{l}$ be a pruned weight matrix for layer $l$. The compression
ratio of layer $l$ is defined as 
\[
\gamma_{l}:=\frac{||\text{vec}(W_{l})||_{0}}{D_{l}}
\]
\item $f(x)$ is said to be $\epsilon$-close to $F(x)$ iff 
\[
\sup_{x\in\mathcal{B}_{d_{0}}}||f(x)-F(x)||<\epsilon
\]
where $\mathcal{B}_{d_{0}}:=\left\{ x\in\mathbb{R}^{d_{0}}\ |\ ||x||_{2}\leq1\right\} $
denotes the $d_{0}$ unit-ball with the Euclidean norm.
\end{itemize}

\subsection*{Intuition Behind the Objects}

In general the objects defined above are either defined to formally
express a pruned network, or to provide some measure of efficacy for
a given pruned network. In the former category are mask matrices,
pruned weight matrices and prunings of the target network. The latter
consists of $\epsilon$-closeness and the compression ratio.

Intuitively, the mask matrix can be thought of as a ``switch'' which
allows or blocks certain components of the previous layer to modify
the input of the activation's output. This can be seen with a toy
example. Suppose that for some layer $l$ of a FCN, the weight function
is given as follows
\[
W_{l}^{*}=\left[\begin{array}{cc}
1 & 2\\
4 & 5\\
7 & 8
\end{array}\right]
\]
Now consider the mask matrix 
\[
M_{l}=\left[\begin{array}{cc}
1 & 1\\
0 & 1\\
1 & 0
\end{array}\right]
\]
This mask induces a pruned weight matrix $W_{l}$
\[
W_{l}=\left[\begin{array}{cc}
1 & 1\\
0 & 1\\
1 & 0
\end{array}\right]\circ\left[\begin{array}{cc}
1 & 2\\
4 & 5\\
7 & 8
\end{array}\right]=\left[\begin{array}{cc}
1 & 2\\
0 & 5\\
7 & 0
\end{array}\right]
\]
Now suppose the input from the previous layer is given by $x=[x_{1},x_{2}]^{T}$,
then 
\[
z:=W_{l}x=\left[\begin{array}{c}
x_{1}+2x_{2}\\
5x_{2}\\
7x_{1}
\end{array}\right]\neq\left[\begin{array}{c}
x_{1}+2x_{2}\\
4x_{1}+5x_{2}\\
7x_{1}+8x_{2}
\end{array}\right]=W_{l}^{*}x=:z^{*}
\]
 Hence we see that the activation input is different between the pruned
and unpruned weight matrices. But crucially, the $i^{\text{th}}$
component $x_{i}$ of the weight matrix input $x$ has no influence
of the $j^{\text{th}}$ component $z_{j}$ of the activation input
$z$ if $\left[M_{l}\right]_{ji}=0$, and the influence of the $x_{i}$
on $z_{j}$ is unaltered when $\left[M_{l}\right]_{ji}=0$. This is
what we mean by saying $M_{l}$ switches --- or prunes --- connections
in a weight matrix. Its effect is tantamount to severing a connection
between the inputs of a weight matrix (or, outputs of the previous
layer's activation function) and the inputs of the layer's activation
function. 

Since $W_{l}=M\circ W_{l}^{*}$, $W_{l}$ is naturally interpreted
as the weight matrix of a network that has had its connections pruned
by $M_{l}$. Finally, $f(x)$ has the same same general architecture
as $F(x)$ --- its activation functions, number of layers and nodes\footnote{If every $M_{ji}=0$ for some $i\in d_{l-1}$, then it is clear the
value of $x_{i}^{l}$ will make no contribution to the vector $z$.
However, strictly speaking the $i^{\text{th}}$node of layer $l-1$
is still part of the network --- it's just vestigial.} are the same as $F(x)$. The only difference between $f(x)$ and
$F(x)$ is that the former uses pruned weight vectors, while the latter
uses the original, unpruned weight vectors. In this way it is clear
that $f(x)$ just represents a subnetwork of $F(x)$ which has had
its connections pruned as specified by the matrices $M_{l},\ l\in\{1,...,L\}$.

The discussion above makes it clear that $f(x),M_{l},W_{l}$ are defined
as a means of formally representing a pruned subnetwork of some FCN.
However, we ultimately wish to describe the efficacy of the pruned
network. Of particular imporance are how much ``smaller'' the pruned
subnetwork is compared to its corresponding FCN, and the discrepancies
between the predictions of the pruned and FC network. 

The former notion is captured by the compression ratio $\gamma_{l}$.
In particular this ratio can be interpreted as the percentage of connections
that are left in the neural network after pruning. So if $\gamma_{l}=0.18$,
only $18\%$ of the original connections between layers $l$ and $l-1$
remain. This can be seen in the extreme cases where a masking matrix
$M_{l}$ for some FC weight matrix $W_{l}^{*}$ has entries that are
identically $0$ or $1$. In the first case, it is clear that every
entry of $M_{l},\ \text{vec}(M_{l})$ are zero, so that $||\text{vec}(W_{l})||_{0}=0$
and $\gamma_{l}=||\text{vec}(W_{l})||_{0}/D_{l}=0$. Meanwhile if
$M_{l}$ has all entries equal to $1$ then $||\text{vec}(M_{l}\circ W_{l}^{*})||_{0}=d_{l}d_{l-1}=D_{k}$
so $\gamma_{l}=1$. Every other case falls in between these two extremes,
since $0\leq||\text{vec}(W_{l})||_{0}\leq D_{k}$ clearly. 

Returning to our toy example, we see that 
\[
W_{l}=\left[\begin{array}{cc}
1 & 2\\
0 & 5\\
7 & 0
\end{array}\right]\Rightarrow||\text{vec}(W_{l})||_{0}=3\Rightarrow\gamma_{l}=\frac{||\text{vec}(W_{l})||_{0}}{d_{l}\cdot d_{l-1}}=\frac{3}{3\cdot2}=0.5
\]
which again describes the percentage of connections which have not
been pruned. 

Finally $\epsilon$-closeness can be interpreted as a guaranteed accuracy.
By selecting $x\in\mathcal{B}_{d_{0}}$ that causes the largest discrepancy
between the values of $f(\cdot)$ and $F(\cdot)$, $f(\cdot)$ and
$F(\cdot)$ are $\epsilon$-close if even in the worst case scenario
the discrepancy between the $f(\cdot)$ and $F(\cdot)$ is less than
$\epsilon$.

With this intuition in mind, the goal of a pruning algorithm can be
cast in a more technical form: given a FCN $F(\cdot)$, find a set
of masking matrices $\{M_{1},...,M_{l}\}$ so that for a given $\epsilon$
the corresponding pruned network $f(\cdot)$ has the smallest possible
compression ratios $\{\gamma_{1},...,\gamma_{l}\}$ while remaining
$\epsilon$-close to $F(\cdot)$.

\subsection*{The Main Result: Magnitude Based Pruning}

Specifying a pruned subnetwork $f(\cdot)$ is tantamount to determining
a set of masking matrices. One approach for selecting $M$ considered
by the authors is so-called ``magnitude based pruning''. This method
begins by ordering the entries of a FC weight matrix $W_{l}^{*}$
by magnitude
\[
|W_{l}^{*}|_{i_{1},j_{1}}\leq\cdots\leq|W_{l}^{*}|_{i_{d_{l}},j_{d_{l}}}
\]
where $i_{n},j_{n}$ refer to the index of the entry in $W_{l}^{*}$
with the $n^{\text{th}}$ smallest magnitude ($n\in\{1,...,D_{k}\}$).
Then $M_{l}$ is set by first specifying the desired compression ratio
$\gamma_{l}$, setting the entries of $M_{l}$ corresponding to the
smallest $\lfloor\gamma_{l}D_{l}\rfloor$ components of $W_{l}^{*}$
to zero, and the rest to one. 

With this context, we state the main result of the paper:\\
\\
\textbf{Theorem 1 of }\cite{qian2021probabilistic}\textbf{:}\emph{
Suppose that $F$ is a FC target network and that:}
\begin{description}
\item [{\emph{(i)}}] \emph{$\sigma_{l}$ is Lipschitz continuous with constant
$K_{l},\ \forall l\in\{1,...,L\}$}
\item [{\emph{(ii)}}] \emph{$d:=\min\{d_{1},...,d_{L-1}\}\geq\max\{d_{0},d_{L}\}$}
\item [{\emph{(iii)}}] \emph{The entries in $W_{k}^{*}$ are iid following
\[
\left[W_{k}^{*}\right]_{i,j}\sim\mathcal{U}\left[-\frac{K}{\sqrt{\max\{d_{l},d_{l-1}\}}},\frac{K}{\sqrt{\max\{d_{l},d_{l-1}\}}}\right];\ i\in\{1,...,d_{l}\},i\in\{1,...,d_{l-1}\}
\]
where $K$ is a fixed positive constant. }
\end{description}
\emph{Let $\epsilon,\delta>0,\alpha\in(0,1)$ so that 
\[
d\geq\max\left\{ C_{1}^{\frac{1}{\alpha}},\left(\frac{C_{2}}{\epsilon}\right)^{\frac{1}{\alpha}},\left(\frac{C_{3}}{\delta}\right)^{\frac{1}{\alpha}},C_{4}+C_{5}\log\left(\frac{1}{\delta}\right)\right\} 
\]
For $C_{1},...,C_{5}$ depending on values of $l,K_{l}$. Then with
probability at least $1-\delta$, the subnetwork $f$ of $F$ with
mask $M=\left\{ M_{1},...,M_{L}\ |\ M_{l}\in\{0,1\}^{d_{l}\times d_{l-1}}\right\} $
pruning the smallest $\lfloor D_{l}^{1-\alpha}\rfloor$ entries of
$W_{l}^{*},\ l\in\{1,...,L\}$ based on magnitude is $\epsilon$-close
to $F$, i.e.
\[
\sup_{x\in\mathcal{B}_{d_{0}}}||f(x)-F(x)||_{2}\leq\epsilon
\]
}

We note that $\alpha$ effectively specifies $\gamma_{l}=D_{l}^{-\alpha}$,
so although it is not directly stated in the theorem, the compression
ratio is present. 

\subsection*{Unpacking and Justifying the Assumptions: }

As previously stated we will not provide a full characterization of
the proof. Instead, we clarify why the authors made the three assumptions
that appear in the theorem. 

First, we comment on the Lipschitz continuity of $\sigma_{k}$. For
our purposes function $g:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$
is Lipschitz continuous iff 
\[
||g(x)-g(y)||_{2}\leq K||x-y||_{2}
\]
for any $x,y\in\mathbb{R}^{n}$ (see \cite{metric_spaces} for more
details on Lipschitz continuity). The intuition when $g:\mathbb{R}\rightarrow\mathbb{R}$
is that $g$ is Lipschitz just when its value never changes quicker
than the function $h(x)=Kx$. The reason the activation functions
are Lipschitz continuous is just because the proof of the main theorem
relies on random matrix theory and, in particular, the probability
that the 2-norms of random matrices exceed certain bounds. By ensuring
that 
\[
||\sigma(z_{l}^{*})-\sigma(z_{l})||_{2}\leq K_{l}||W_{l}^{*}x_{l}-W_{l}x_{l}||_{2}
\]
the authors are able to extend the results of bounding the difference
between vectors under random matrices, to bounding the difference
between vectors under non-linear transformations (see section 4 of
\cite{qian2021probabilistic}). So, the results of random matrix theory
can be harnessed to prove the main results.

The second assumption is largely used to provide tighter bounds on
the probabilities given in the relevant theorem. As such its role
is mostly related to technical details (again see section 4 of \cite{qian2021probabilistic}).

Finally, the third assumption is by far the most important assumption.
As previously mentionned, the main proof relies on a few theorems
imported from random matrix theory \cite{qian2021probabilistic}.
In particular, the main theorem heavily relies on \cite{doi:https://doi.org/10.1002/0471667196.ess6023,10.2307/4097777},
of which the former only applies to independent, mean zero random
matrices with finite fourth moments and the latter which applies to
(sub-Gaussian) iid random matrices.

The first two assumptions are easy to justify. For the first assumption,
we note that in practice most activation functions are Lipschitz continuous.
For instance, the sigmoid, ReLU, tanh and softmax functions are all
Lipschitz continuous with $K=1$ (see \cite{Lipschitz}). So this
is clearly a reasonable assumption. The second assumption is even
easier to justify. One can simply set all $d\geq\max\{d_{0},d_{l}\}$
when designing the architecture of the FCN $F(\cdot)$. Hence the
first two assumptions will almost always hold in practice. 

Unlike the first two assumptions, the third assumption is not evidently
the case. Assuming that $\{W_{l}^{*}\ |\ l\in\{1,...,L\}\}$ are i.i.d.
random is a priori strong assumption with no particularly strong justification.
Given this difficulty, the authors essentially argue that because
(1) there is no way to effectively measure how close weights are to
independence, (2) some literature --- namely \cite{jacot_gabriel_hongler_2021,bai2020linearization}---
suggests that weights in a trained network do not deviate strongly
from their initial values. However, in our view it is clear that this
constitutes only a partial, if not promising, justification for assumption
(iii). Consequently the most important assumption for the main theorem
is also the least secure. 

\newpage{}

\bibliographystyle{plain}
\bibliography{Bib}

\end{document}
